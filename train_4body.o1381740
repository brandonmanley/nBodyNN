/users/PAS1585/llavez99/work/nbody/nBodyNN
Using TensorFlow backend.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
2020-04-06 16:02:41.653316: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-04-06 16:02:41.663675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2020-04-06 16:02:41.664117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5652a85d6d70 executing computations on platform Host. Devices:
2020-04-06 16:02:41.664141: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-04-06 16:02:41.751607: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
/users/PAS1585/llavez99/work/nbody/nBodyNN/nn_kfold_train_4body.py:75: FutureWarning:

Method .as_matrix will be removed in a future version. Use .values instead.

/users/PAS1585/llavez99/work/nbody/nBodyNN/nn_kfold_train_4body.py:76: FutureWarning:

Method .as_matrix will be removed in a future version. Use .values instead.

WARNING:tensorflow:From /users/PAS1585/llavez99/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /users/PAS1585/llavez99/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

   file  eventID         m1  ...      dyf3      dyf4  Unnamed: 39
0     1    10001  15.493888  ...  0.445991 -0.566522          NaN
1     1    10002  15.493888  ...  0.443628 -0.564230          NaN
2     1    10003  15.493888  ...  0.441266 -0.561939          NaN
3     1    10004  15.493888  ...  0.438906 -0.559649          NaN
4     1    10005  15.493888  ...  0.436546 -0.557361          NaN

[5 rows x 40 columns]
   file  eventID         m1         m2  ...      dyf1      dyf2      dyf3      dyf4
0     1    10001  15.493888  35.570682  ...  0.789834 -0.910954  0.445991 -0.566522
1     1    10002  15.493888  35.570682  ...  0.793880 -0.912933  0.443628 -0.564230
2     1    10003  15.493888  35.570682  ...  0.797931 -0.914913  0.441266 -0.561939
3     1    10004  15.493888  35.570682  ...  0.801988 -0.916894  0.438906 -0.559649
4     1    10005  15.493888  35.570682  ...  0.806052 -0.918877  0.436546 -0.557361

[5 rows x 39 columns]
(1223314, 21) (1223314, 16)
(12357, 21) (12357, 16)
Training:
Layers: 4
Nodes: 50
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.9449608027935028 8.557638009187988 100.0
Training:
Layers: 4
Nodes: 128
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.2476481795310974 5.394845243188643 100.0
Training:
Layers: 4
Nodes: 200
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.091851145029068 4.777647435566899 100.0
Training:
Layers: 4
Nodes: 300
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
0.9909170418977737 4.385916774920598 100.0
Training:
Layers: 9
Nodes: 50
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.7346945703029633 7.487363382042807 100.0
Training:
Layers: 9
Nodes: 128
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.1101557612419128 4.767449660924153 100.0
Training:
Layers: 9
Nodes: 200
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
0.9310942590236664 4.114286542259263 100.0
Training:
Layers: 9
Nodes: 300
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
0.8660521656274796 3.865424540742186 100.0
Training:
Layers: 14
Nodes: 50
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.861320823431015 8.05723770442978 100.0
Training:
Layers: 14
Nodes: 128
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.0663712918758392 4.534541939678191 100.0
Training:
Layers: 14
Nodes: 200
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
0.9316174387931824 4.053035403743288 100.0
Training:
Layers: 14
Nodes: 300
Training on numSplit: 0
Training on numSplit: 1
Training on numSplit: 2
Training on numSplit: 3
1.0313267409801483 4.450484676381247 100.0
[[4, 50], [4, 128], [4, 200], [4, 300], [9, 50], [9, 128], [9, 200], [9, 300], [14, 50], [14, 128], [14, 200], [14, 300]]
[1.9449608027935028, 1.2476481795310974, 1.091851145029068, 0.9909170418977737, 1.7346945703029633, 1.1101557612419128, 0.9310942590236664, 0.8660521656274796, 1.861320823431015, 1.0663712918758392, 0.9316174387931824, 1.0313267409801483]
[8.557638009187988, 5.394845243188643, 4.777647435566899, 4.385916774920598, 7.487363382042807, 4.767449660924153, 4.114286542259263, 3.865424540742186, 8.05723770442978, 4.534541939678191, 4.053035403743288, 4.450484676381247]
[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]
****** p0061.ten.osc.edu: Excessive memory usage detected; job may have failed. ******

-----------------------
Resources requested:
nodes=1:ppn=10
mem=5000mb
-----------------------
Resources used:
cput=63:06:56
walltime=10:16:27
mem=2.981GB
vmem=7.438GB
-----------------------
Resource units charged (estimate):
10.274 RUs
-----------------------
