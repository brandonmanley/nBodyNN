{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456882, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365505, 11) (365505, 21)\n",
      "(91377, 11) (91377, 21)\n",
      "(365505, 20) (91377, 20)\n"
     ]
    }
   ],
   "source": [
    "workDir = \"/Users/brandonmanley/Documents/nBody/data/\"\n",
    "dataDir = \"/Users/brandonmanley/Documents/nBody/data/brutusSim/\"\n",
    "\n",
    "#Import data\n",
    "# df = util.concatCSV(dataDir+'batch3')\n",
    "df = pd.read_csv(dataDir + \"brutus10_1_2.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "dfShuffle = shuffle(df,random_state=42)\n",
    "\n",
    "i_col = [\"m1\",\"m2\", \"x1\", \"x2\", \"y1\", \"y2\",\n",
    "\t\t\"dx1\",\"dx2\",\"dy1\",\"dy2\",\"tEnd\"]\n",
    "o_col = [\"x1f\", \"x2f\", \"y1f\", \"y2f\",\n",
    "\t\t\"dx1f\", \"dx2f\", \"dx3f\", \"dy1f\", \"dy2f\",\n",
    "\t\t\"eventID\"]\n",
    "\n",
    "X1 = dfShuffle.as_matrix(columns=i_col)\n",
    "y1 = dfShuffle.as_matrix(columns=i_col+o_col)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n",
    "#extract id list from the y arrays\n",
    "id_list_train = y_train[:,len(i_col+o_col)-1]\n",
    "id_list_test = y_test[:,len(i_col+o_col)-1]\n",
    "y_train = np.delete(y_train,len(i_col+o_col)-1,1)\n",
    "y_test = np.delete(y_test,len(i_col+o_col)-1,1)\n",
    "\n",
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "y_train = y_train.astype('float64')\n",
    "y_test = y_test.astype('float64')\n",
    "\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_mse(y_true,y_pred): # FIXME: Update for 2 bodies\n",
    "\n",
    "    masses = y_true[:, 2]\n",
    "\n",
    "    #initial positions\n",
    "    p0 = y_true[:, 3:8] \n",
    "\n",
    "    #final positions and velocities\n",
    "    y_true = y_true[:,10:]\n",
    "\n",
    "    #predicted final positions and velocities\n",
    "    p1 = y_pred\n",
    "\n",
    "    #mean squared error between predicted and true\n",
    "    mse = K.mean(K.square(y_pred-y_true),axis=-1)\n",
    "\n",
    "    #intial and final CM, delta\n",
    "    cm_x_i = (masses[0]*p0[0]+masses[1]*p0[1]+masses[2]*p0[2])/(masses[0]+masses[1]+masses[2])\n",
    "    cm_y_i = (masses[0]*p0[3]+masses[1]*p0[4]+masses[2]*p0[4])/(masses[0]+masses[1]+masses[2])\n",
    "    cm_x_f = (masses[0]*p1[0]+masses[1]*p1[1]+masses[2]*p1[2])/(masses[0]+masses[1]+masses[2])\n",
    "    cm_y_f = (masses[0]*p1[3]+masses[1]*p1[4]+masses[2]*p1[4])/(masses[0]+masses[1]+masses[2])\n",
    "    delta_cm_x = abs(cm_x_i-cm_x_f)\n",
    "    delta_cm_y = abs(cm_y_i-cm_y_f)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 365505 samples, validate on 91377 samples\n",
      "Epoch 1/30\n",
      "365505/365505 [==============================] - 17s 48us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 2/30\n",
      "365505/365505 [==============================] - 18s 49us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 3/30\n",
      "365505/365505 [==============================] - 18s 50us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 4/30\n",
      "365505/365505 [==============================] - 16s 43us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 5/30\n",
      "365505/365505 [==============================] - 13s 37us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 6/30\n",
      "365505/365505 [==============================] - 14s 39us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 7/30\n",
      "365505/365505 [==============================] - 14s 38us/step - loss: nan - accuracy: 0.4952 - val_loss: nan - val_accuracy: 0.4943\n",
      "Epoch 8/30\n",
      "106112/365505 [=======>......................] - ETA: 9s - loss: nan - accuracy: 0.4923"
     ]
    }
   ],
   "source": [
    "#parameters \n",
    "max_epochs = 30\n",
    "optimizer = 'adam'\n",
    "batch_size = 128         #FIXME: paper used 5000 for 10000 events\n",
    "loss_function = 'mse'    #or modified_mse\n",
    "\n",
    "#early stopping\n",
    "patienceCount = 20\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=patienceCount),\n",
    "             ModelCheckpoint(filepath=workDir+'/weights/best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(128,activation='relu',input_dim=len(i_col)))\n",
    "for i in range(9):\n",
    "    network.add(layers.Dense(128,activation='relu'))\n",
    "network.add(layers.Dense(20,activation='linear'))\n",
    "network.compile(optimizer=optimizer,loss=loss_function,metrics=['accuracy'])\n",
    "network.save_weights(workDir + '/weights/model_init.h5')\n",
    "\n",
    "history = network.fit(X_train,y_train,\n",
    "                              epochs=max_epochs,\n",
    "                              callbacks = callbacks,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(X_test,y_test),\n",
    "                              verbose = 1)\n",
    "\n",
    "training_vals_acc = history.history['accuracy']\n",
    "training_vals_loss = history.history['loss']\n",
    "valid_vals_acc = history.history['val_accuracy']\n",
    "valid_vals_loss = history.history['val_loss']\n",
    "iterations = len(training_vals_acc)\n",
    "print(\"Number of iterations:\",iterations)\n",
    "print(\"Epoch\\t Train Loss\\t Train Acc\\t Val Loss\\t Val Acc\")\n",
    "i = 0\n",
    "for tl,ta,vl,va in zip(training_vals_loss,training_vals_acc,valid_vals_loss,valid_vals_acc):\n",
    "    print(i,'\\t',round(tl,5),'\\t',round(ta,5),'\\t',round(vl,5),'\\t',round(va,5))\n",
    "    i += 1\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "print(history.history.keys())\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(workDir + 'model_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig(workDir + 'model_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(X_test)\n",
    "\n",
    "pred_out = np.asarray(predictions)\n",
    "id_list_test = np.reshape(id_list_test,(id_list_test.shape[0],1))\n",
    "pred_out = np.concatenate((pred_out,id_list_test),axis=1)\n",
    "np.savetxt(workDir+\"predicted_paths_10_2.csv\", pred_out, delimiter=\",\")\n",
    "\n",
    "sim_out = np.asarray(y_test)\n",
    "sim_out = np.concatenate((sim_out,id_list_test),axis=1)\n",
    "np.savetxt(workDir+\"sim_paths_10_2.csv\", pred_out, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
